{"origin_pdf_path": "https://arxiv.org/pdf/1709.09844", "text_in_pdf": "Distance-based Conﬁdence Score for Neural Network Classiﬁers\nAmit Mandelbaum Daphna Weinshall\nSchool of Computer Science and Engineering\nHebrew University of Jerusalem, IsraelSchool of Computer Science and Engineering\nHebrew University of Jerusalem, Israel\nAbstract\nThe reliable measurement of conﬁdence in\nclassiﬁers’ predictions is very important for\nmany applications, and is therefore an im-\nportant part of classiﬁer design. Yet, al-\nthough deep learning has received tremen-\ndous attention in recent years, not much\nprogress has been made in quantifying the\nprediction conﬁdence of neural network classi-\nﬁers. Bayesian models oﬀer a mathematically\ngrounded framework to reason about model\nuncertainty, butusuallycomewithprohibitive\ncomputational costs. In this paper we pro-\npose a simple, scalable method to achieve a\nreliable conﬁdence score, based on the data\nembedding derived from the penultimate layer\nof the network. We investigate two ways to\nachieve desirable embeddings, by using either\na distance-based loss or Adversarial Train-\ning. We then test the beneﬁts of our method\nwhen used for classiﬁcation error prediction,\nweighting an ensemble of classiﬁers, and nov-\nelty detection. In all tasks we show signiﬁcant\nimprovement over traditional, commonly used\nconﬁdence scores.\n1 Introduction\nClassiﬁcation conﬁdence scores are designed to mea-\nsure the accuracy of the model when predicting class\nassignment (rather than the uncertainty inherent in the\ndata). Most generative classiﬁcation models are proba-\nbilistic in nature, and therefore provide such conﬁdence\nscores directly. Most discriminative models, on the\nother hand, do not have direct access to the probability\nof each prediction. Instead, related non-probabilisticscores are used as proxies, as for example the margin\nin SVM classiﬁers.\nWhen trying to evaluate the conﬁdence of neural net-\nwork (NN) classiﬁers, a number of scores are commonly\nused. One is the strength of the most activated output\nunit followed by softmax normalization, or the closely\nrelated ratio between the activities of the strongest\nand second strongest units. Another is the (negative)\nentropy of the output units, which is minimal when\nall units are equally probable. Often, however, these\nscores do not provide a reliable measure of conﬁdence.\nWhy is it important to reliably measure prediction con-\nﬁdence? In various contexts such as medical diagnosis\nand decision support systems, it is important to know\nthe prediction conﬁdence in order to decide how to act\nupon it. For example, if the conﬁdence in a certain\nprediction is too low, the involvement of a human ex-\npert in the decision process may be called for. Another\nimportant aspect of real world applications is the abil-\nity to recognize samples that do not belong to any of\nthe known classes, which can also be improved with a\nreliable conﬁdence score. But even irrespective of the\napplication context, reliable prediction conﬁdence can\nbe used to boost the classiﬁer performance via such\nmethods as self-training or ensemble classiﬁcation. In\nthis context a better conﬁdence score can improve the\nﬁnal performance of the classiﬁer. The derivation of a\ngood conﬁdence score should therefore be part of the\nclassiﬁer’s design, as important as any other component\nof classiﬁers’ design.\nIn order to derive a reliable conﬁdence score for NN\nclassiﬁers, we focus our attention on an empirical obser-\nvation concerning neural networks trained for classiﬁca-\ntion, which have been shown to demonstrate in parallel\nuseful embedding properties. Speciﬁcally, a common\npractice these days is to treat one of the upstream\nlayers of a pre-trained network as a representation (or\nembedding) layer. This layer activation is then used for\nrepresenting similar objects and train simpler classiﬁers\n(such as SVM, or shallower NNs) to perform diﬀerent\ntasks, related but not identical to the original task the\nnetwork had been trained on.arXiv:1709.09844v1  [cs.AI]  28 Sep 2017\n\nDistance-based Conﬁdence Score for Neural Network Classiﬁers\nIn computer vision such embeddings are commonly\nobtained by training a deep network on the recognition\nof a very large database (typically ImageNet (Deng\net al., 2009)). These embeddings have been shown to\nprovide better semantic representations of images (as\ncompared to more traditional image features) in a num-\nber of related tasks, including the classiﬁcation of small\ndatasets (Sharif Razavian et al., 2014), image annota-\ntion (Donahue et al., 2015) and structured predictions\n(Hu et al., 2016). Given this semantic representation,\none can compute a natural multi-class probability dis-\ntribution as described in Section 2.1, by estimating\nlocal density in the embedding space. This estimated\ndensity can be used to assign a conﬁdence score to each\ntest point, using its likelihood to belong to the assigned\nclass.\nWe note, however, that the commonly used embedding\ndiscussed above is associated with a network trained\nfor classiﬁcation only, which may impede its suitability\nto measure conﬁdence reliably. In fact, when train-\ning neural networks, metric learning is often used to\nachievedesirableembeddings(e.g., Westonetal.(2012);\nSchroﬀ et al. (2015); Hoﬀer & Ailon (2015); Tadmor\net al. (2016)). Since our goal is to improve the prob-\nabilistic interpretation of the embedding, which is es-\nsentially based on local point density estimation (or\nthe distance between points), we may wish to modify\nthe loss function and add a term which penalizes for\nthe violation of pairwise constraints as in Hadsell et al.\n(2006). Our experiments show that the modiﬁed net-\nwork indeed produces a better conﬁdence score, with\ncomparable classiﬁcation performance. Surprisingly,\nwhile not directly designed for this purpose, we show\nthat networks which are trained with adversarial ex-\namples following the Adversarial Training paradigm\n(Szegedy et al., 2013; Goodfellow et al., 2014), also\nprovide a suitable embedding for the new conﬁdence\nscore.\nOur ﬁrst contribution, therefore, is a new prediction\nconﬁdence score which is based on local density esti-\nmation in the embedding space of the neural network.\nThis score can be computed for every network, but in\norder for this score to achieve superior performance, it\nis necessary to slightly change the training procedure.\nIn our second contribution we show that suitable em-\nbedding can be achieved by either augmenting the loss\nfunction of the trained network with a term which pe-\nnalizes for distance-based similarity loss (as in Eq. (2)\nbelow), or by using Adversarial Training. The impor-\ntance of the latter contribution is two fold: Firstly, we\nare the ﬁrst to show that the density of image embed-\ndings is improved with indirect Adversarial Training\nperturbations, in addition to the improved word em-\nbedding quality shown in Miyato et al. (2016) by directAdversarial Training perturbations. Secondly, we show\nin Section 3 that Adversarial Training improves the\nresults while imposing a much lighter burden of hyper-\nparameters to tune as compared to the distance-based\nloss.\nThe new conﬁdence score is evaluated in comparison\nto other scores, using the following tasks: (i) Perfor-\nmance in the binary classiﬁcation task of identifying\neach class prediction as correct or incorrect (see Sec-\ntion 2.1). (ii) Training an ensemble of NN classiﬁers,\nwhere each classiﬁer’s prediction is weighted by the\nnew conﬁdence score (see Section 2.4). (iii) Novelty\ndetection, where conﬁdence is used to predict whether\na test point belongs to one of the known classes from\nthe train set (see Section 2.5).\nThe empirical evaluation of our method is described\nin Section 3, using a few datasets and diﬀerent net-\nwork architectures which have been used in previous\nwork when using these speciﬁc datasets. Our method\nachieves signiﬁcant improvement in all 3 tasks. When\ncompared with a more recent method which had been\nshown to improve traditional measures of classiﬁcation\nconﬁdence - MC dropout (Gal & Ghahramani, 2015),\nour distance-based score achieves better results while\nalso maintaining lower computational costs.\nPrior Work\nThe Bayesian approach seeks to compute a posterior\ndistribution over the parameters of the neural network\nwhich is used to estimate prediction uncertainty, as in\nMacKay (1992) and Neal (2012). However, Bayesian\nneural networks are not always practical to implement,\nand the computational cost involved it typically high.\nIn accordance, in a method which is referred to below\nasMC-Dropout , Gal & Ghahramani (2015) proposed\nto use dropout during test time as a Bayesian approxi-\nmation of the neural network, providing a cheap proxy\nto Bayesian Neural Networks. Lakshminarayanan et al.\n(2016) proposed to use Adversarial Training to improve\nthe uncertainty measure of the entropy score of the\nneural network.\nStill, the most basic and one of the most common\nconﬁdence scores for neural networks can be derived\nfrom the strength of the most activated output unit, or\nratheritsnormalizedversion(alsocalled softmax output\normax margin ). A conﬁdence score that handles better\na situation where there is no one class which is most\nprobable, is the (negative) entropy of the normalized\nnetwork’s output. Zaragoza & d’Alché Buc (1998)\ncompared these scores, as well as some more complex\nones (e.g. Tibshirani (1996)), demonstrating somewhat\nsurprisingly the empirical superiority of the two most\nbasic methods described in the previous paragraph.\n\nAmit Mandelbaum, Daphna Weinshall\nEnsembles of models have been used to improve the\noverall performance of the ﬁnal classiﬁer (see reviews\nin Dietterich (2000) and Li et al. (2017)). There are\nmany ways to train an ensemble, such as boosting or\nbagging. There are also many ways to integrate the\npredictions of the classiﬁers in the ensemble, including\nthe average prediction or voting discussed by Bauer\n& Kohavi (1999). Some ensemble methods use the\nconﬁdence score to either weight the predictions of the\ndiﬀerent classiﬁers (average weighting) or for conﬁdence\nvoting.\nNovelty detection, where the task is to determine\nwhether a test point belongs to a known class label\nor not, is another problem which becomes more rele-\nvant with the ever increasing availability of very large\ndatasets, see reviews in Markou & Singh (2003), Pi-\nmentel et al. (2014) and the recent work in Vinokurov\n& Weinshall (2016). This task is also highly relevant\nin real world applications, where the classiﬁer is usu-\nally exposed to many samples which do not belong to\na known class. Note that novelty detection is quite\ndiﬀerent from the learning of classes with no examples,\nas in zero shot learning (Palatucci et al., 2009).\n2 New Conﬁdence Score\nWe propose next a new conﬁdence score. We then\ndiscuss how it can be used to boost classiﬁcation per-\nformance with ensemble methods, or when dealing with\nnovelty detection.\n2.1 New Conﬁdence Score for Neural\nNetwork Classiﬁers\nOur conﬁdence score is based on the estimation of local\ndensity as induced by the network, when points are\nrepresented using the eﬀective embedding created by\nthe trained network in one of its upstream layers. Local\ndensity at a point is estimated based on the Euclidean\ndistance in the embedded space between the point and\nitsknearest neighbors in the training set.\nSpeciﬁcally, let f(x)denote the embedding of xas\ndeﬁned by the trained neural network classiﬁer. Let\nA(x) =fxj\ntraingk\nj=1denote the set of k-nearest neigh-\nbors ofxin the training set, based on the Euclidean\ndistance in the embedded space, and let fyjgk\nj=1denote\nthe corresponding class labels of the points in A(x). A\nprobability space is constructed (as is customary) by\nassuming that the likelihood that two points belong to\nthe same class is proportional to the exponential of the\nnegative Euclidean distance between them. In accor-\ndance, the local probability that a point xbelongs to\nclasscis proportional to the probability that it belongs\nto the same class as the subset of points in A(x)thatbelong to class c.\nBased on this local probability, the conﬁdence score\nD(x)for the assignment of point xto class ^yis deﬁned\nas follows:\nD(x) =Pk\nj=1;yj=^ye\u0000jjf(x)\u0000f(xj\ntrain)jj2\nPk\nj=1e\u0000jjf(x)\u0000f(xj\ntrain)jj2(1)\nD(x)is a score between 0 to 1, which is monotonically\nrelated to the local density of similarly labeled train\npoints in the neighborhood of x. Henceforth (1) is\nreferred to as Distance score .1We note here that while\nintuitively it might be beneﬁcial to add a scaling factor\nto the distance in (1), such as the mean distance, we\nfound it to have a deteriorating eﬀect, in line with\nrelated work such as Salakhutdinov & Hinton (2007).\nTwo ways to achieve eﬀective embedding: As\nmentioned is Section 1, in order to achieve an eﬀective\nembedding it helps to modify the training procedure of\nthe neural network classiﬁer. The simplest modiﬁcation\naugments the network’s loss function during training\nwith an additional term. The resulting loss function is\na linear combination of two terms, one for classiﬁcation\ndenotedLclass(X;Y), and another pairwise loss for\nthe embedding denoted Ldist(X;Y). This is deﬁned as\nfollows:\nL(X;Y) =Lclass(X;Y) +\u000bLdist(X;Y)(2)\nLdist(X;Y) =1\nPPX\np=1Ldist(xp1;xp2)\nwhereLdist(xi;xj)is deﬁned as\n(\njjf(xi)\u0000f(xj)jj2 ifyi=yj\nmaxf0;(m\u0000jjf(xi)\u0000f(xj)jj2)gifyi6=yj\nA desirable embedding can also be achieved by Ad-\nversarial Training, using the fast gradient method sug-\ngested in Goodfellow et al. (2014). In this method,\ngiven an input xwith target y, and a neural network\nwith parameters \u0012, adversarial examples are generated\nusing:\nx0=x+\u000f sign (5xLclass(\u0012;x;y )) (3)\nIn each step an adversarial example is generated for\neach pointxin the batch and the current parameters\nof the network, and classiﬁcation loss is minimized for\nboth the regular and adversarial examples. Although\noriginally designed to improve robustness, this method\n1Related measures of density, such as a count of the\n\"correct\" neighbors or the inverse of the distance, behave\nsimilarly and perform comparably.\n\nDistance-based Conﬁdence Score for Neural Network Classiﬁers\nseems to improve the network’s embedding for the pur-\npose of density estimation, possibly because along the\nway it increases the distance between pairs of adjacent\npoints with diﬀerent labels.\nImplementation details: In (2)Ldistis deﬁned by\nall pairs of points, denoted (xp1;xp2). For each training\nminibatch, thissetissampledwithnoreplacementfrom\nthe training points in the minibatch, with half as many\npairs as the size of the minibatch. In our experiments,\nLclass(X;Y)is the regular cross entropy loss. We note\nhere that we also tried distance-based loss functions\nwhich do not limit the distance between points of the\nsame class to be exactly 0 (such as those in Hoﬀer\n& Ailon (2015) and Tadmor et al. (2016)). However,\nthose functions produced worse results, especially when\nthe dataset had many classes. Finally we note that we\nhave tried using the distance-based loss and adversarial\ntraining together while training the network, but this\nhas also produced worse results.\n2.2 Alternative conﬁdence scores\nGiven a trained network, two measure are usually used\nto evaluate classiﬁcation conﬁdence:\nMax margin: the maximal activation, after normal-\nization, in the output layer of the network.\nEntropy: the (negative) entropy of the activations in\nthe output layer of the network.\nAs noted above, the empirical study in Zaragoza &\nd’Alché Buc (1998) showed that these two measures\nare typically as good as any other existing method for\nthe evaluation of classiﬁcation conﬁdence.\nTwo recent methods have been shown to improve the re-\nliability of the conﬁdence score based on Entropy: MC-\nDropout (Gal & Ghahramani, 2015) and Adversarial\nTraining (Lakshminarayanan et al., 2016; Goodfellow\net al., 2014). In terms of computational cost, adversar-\nial training can increase (and sometimes double) the\ntraining time, due to the computation of additional\ngradients and the addition of the adversarial examples\nto the training set. MC-Dropout, on the other hand,\ndoes not change the training time but increases the test\ntime by orders of magnitude (typically 100-fold). Both\nmethods are complementary to our approach, in that\nthey focus on modiﬁcations to the actual computation\nof the network during either train or test time. After\nall is done, they both evaluate conﬁdence using the\nEntropy score. As we show in our experiments, adver-\nsarial training combined with our proposed conﬁdence\nscore improves the ﬁnal results signiﬁcantly.2.3 Our method: computational analysis\nUnlike the two methods described above, MC-Dropout\nandAdversarial Training , our distance-based conﬁ-\ndence score takes an existing network and computes\na new conﬁdence score from the network’s embedding\nand output activation. It can use any network, with\nor without adversarial training or MC dropout. If the\nloss function of the network is suitably augmented (see\ndiscussion above), empirical results in Section 3 show\nthat our score always improves results over the Entropy\nscore of the given network.\nTrain and test computational complexity: Con-\nsidering the distance-based loss, Tadmor et al. (2016)\nshowed that computing distances during the training\nof neural networks have negligible eﬀect on training\ntime. Alternatively, when using adversarial training,\nadditional computational cost is incurred as mentioned\nabove, while on the other hand fewer hyper parame-\nters are left for tuning. During test time, our method\nrequires carrying over the embeddings of the train-\ning data and also the computation of the knearest\nneighbors for each sample.\nNearest neighbor classiﬁcation has been studied exten-\nsively in the past 50 years, and consequently there are\nmany methods to perform either precise or approxi-\nmatek-nn with reduced time and space complexity (see\nGunadi (2011) for a recent empirical comparison of the\nmain methods). In our experiments, while using either\nCondensed Nearest Neighbours (Hart, 1968) or Density\nPreserving Sampling (Budka & Gabrys, 2013), we were\nable to reduce the memory requirements of the train set\nto5%of its original size without aﬀecting performance.\nAt this point the additional storage required for the\nnearest neighbor step was much smaller than the size\nof the networks used for classiﬁcation, and the increase\nin space complexity became insigniﬁcant.\nWith regards to time complexity, recent studies have\nshown how modern GPU’s can be used to speed up\nnearest neighbor computation by orders of magnitude\n(Garcia et al., 2008; Areﬁn et al., 2012). Hyvönen\net al. (2015) also showed that k-nn approximation with\n99% recall can be accomplished 10-100 times faster as\ncompared to precise k-nn.\nCombining such reductions in both space and time, we\nnote that even for a very large dataset, including for\nexample 1M images embedded in a 1K dimensional\nspace, the computation complexity of the knearest\nneighbors for each test sample requires at most 5M\nﬂoating-point operations. This is comparable and even\nmuchfasterthanasingleforwardrunofthistestsample\nthrough a modern, relatively small, ResNets (He et al.,\n2016) with 2-30M parameters. Thus, our method scales\n\nAmit Mandelbaum, Daphna Weinshall\nwell even for very large datasets.\n2.4 Ensembles of Classiﬁers\nThere are many ways to deﬁne ensembles of classiﬁers,\nand diﬀerent ways to put them together. Here we focus\non ensembles which are obtained when using diﬀerent\ntrainingparameterswithasingletrainingmethod. This\nspeciﬁcally means that we train several neural networks\nusing random initialization of the network parameters,\nalong with random shuﬄing of the train points.\nHenceforth Regular Networks will refer to networks\nthat were trained only for classiﬁcation with the reg-\nular cross-entropy loss, Distance Networks will refer\nto networks that were trained with the loss function\ndeﬁned in (2), and AT Networks will refer to networks\nthat were trained with adversarial examples as deﬁned\nin (3).\nEnsemble methods diﬀer in how they weigh the predic-\ntions of diﬀerent classiﬁers in the ensemble. A number\nof options are in common use (see Li et al. (2017)\nfor a recent review), and in accordance are used for\ncomparison in the experimental evaluation section: 1)\nsoftmax average, 2) simple voting, 3) weighted softmax\naverage (where each softmax vector is multiplied by\nits related prediction conﬁdence score), 4) conﬁdence\nvoting (where the most conﬁdent network getsn\n2votes),\nand 5) dictator voting (the decision of the most con-\nﬁdent network prevails). We evaluate methods 3\u00005\nwith weights deﬁned by either the Entropy score or the\nDistance score deﬁned in (1).\n2.5 Novelty Detection\nNovelty detection seeks to identify points in the test set\nwhich belong to classes not present in the train set. To\nevaluate performance in this task we train a network\nwith a known benchmark dataset, while augmenting\nthe test set with test points from another dataset that\nincludes diﬀerent classes. Each conﬁdence score is used\nto diﬀerentiate between known and unknown samples.\nThis is a binary classiﬁcation task, and therefore we\ncan evaluate performance using ROC curves.\n3 Experimental Evaluation\nIn this section we empirically evaluate the beneﬁts of\nour proposed approach, comparing the performance\nof the new conﬁdence score with alternative existing\nscores in the 3 diﬀerent tasks described above.\n3.1 Experimental Settings\nFor evaluation we used 3 data sets: CIFAR-100\n(Krizhevsky & Hinton, 2009), STL-10 (Coates et al.,2010) ( 32\u000232version) and SVHN (Netzer et al., 2011).\nIn all cases, as is commonly done, the data was pre-\nprocessed using global contrast normalization and ZCA\nwhitening. No other method of data augmentation was\nused for CIFAR-100 and SVHN, while for SVHN we\nalso did not use the additional ˜500K labeled images2.\nFor STL-10, on the other hand, cropping and ﬂipping\nwere used for STL-10 to check the robustness of our\nmethod to heavy data augmentation.\nIn our experiments, all networks used ELU (Clevert\net al., 2015) for non-linear activation. For CIFAR-100\nand STL-10 we used the network suggested in Clevert\net al. (2015) with the following architecture:\nC(192;5))P(2))C(192;1))C(240;3))\nP(2))C(240;1))C(260;3))P(2))\nC(260;1))C(280;2))P(2))C(280;1))\nC(300;2))P(2))C(300;1))FC(100)\nC(n;k)denotes a convolution layer with nkernels of\nsizek\u0002kand stride 1. P(k)denotes a max-pooling\nlayer with window size k\u0002kand stride 2, and FC(n)\ndenotes a fully connected layer with noutput units. For\nSTL-10 the last layer was replaced by FC(10). During\ntraining (only) we applied dropout (Srivastava et al.,\n2014)beforeeachmaxpoolinglayer(excludingtheﬁrst)\nand after the last convolution, with the corresponding\ndrop probabilities of [0:1;0:2;0:3;0:4;0:5].\nWith the SVHN dataset we used the following archi-\ntecture:\nC(32;3))C(32;3))C(32;3))P(2))\nC(64;3))C(64;3))C(64;3))P(2))\nC(128;3))C(128;3))C(128;3))P(2))\nFC(128))FC(10)\nFor the networks trained with distance loss, for each\nbatch, we randomly picked pairs of points so that at\nleast 20% of the batch included pairs of points from\nthe same class. The margin min (2) was set to 25 in\nall cases, and the parameter \u000bin (2) was set to 0.2.\nThe rest of the training parameters can be found in the\nsupplementary material. For the distance score we ob-\nserved that the number of knearest neighbors could be\nset to the maximum value, which is the number of sam-\nples in each class in the train data. We also observed\nthat smaller numbers (even k= 50) often worked as\n2Note that reported results denoted as \"state-of-the-art\"\nfor these datasets often involve heavy augmentation. In our\nstudy, in order to be able to do the exhaustive comparisons\ndescribed below, we opted for the un-augmented scenario\nas more ﬂexible and yet informative enough for the purpose\nof comparison between diﬀerent methods. Therefore our\nnumerical results should be compared to empirical studies\nwhich used similar un-augmented settings . We speciﬁcally\nselected commonly used architectures that achieve good\nperformance, close to the results of modern ResNets, and\nyet ﬂexible enough for extensive evaluations.\n\nDistance-based Conﬁdence Score for Neural Network Classiﬁers\nTable 1: AUC Results of Correct Classiﬁcation.\nConf.\nScoreCIFAR-100\n(Classiﬁer acccuracy: 60%)STL-10\n(Classiﬁer acccuracy: 70.5%)SVHN\n(Accuracy: 93.5%)\nReg. Dist. AT MCD Reg. Dist. AT MCD Reg. Dist. AT\nMargin 0.828 0.834 0.844 0.836 0.804 0.775 0.812 0.804 0.904 0.896 0.909\nEntropy 0.833 0.837 0.851 0.833 0.806 0.786 0.816 0.810 0.916 0.907 0.918\nDistance 0.7890.853 0.843 0.726 0.798 0.824 0.863 0.6710.9160.925 0.925\nTable 1, legend. Leftmost column: MarginandEntropy denote the commonly used conﬁdence scores described in\nSection 2.2. Distance denotes our proposed method described in Section 2.1. Second line: Reg.denotes networks trained\nwith the entropy loss, Dist.denotes networks trained with the distance loss deﬁned in (2), ATdenotes networks trained\nwith adversarial training as deﬁned in (3), and MCDdenotes MC-Dropout when applied to networks normally trained\nwith the entropy loss. Since the network trained for SVHN was trained without dropout, MCDwas not applicable.\nTable 2: AUC Results of Correct classiﬁcation - Ensemble of 2 Networks.\nConﬁdence\nScoreCIFAR-100 STL-10 SVHN\nReg. Dist. AT Reg. Dist. AT Reg. Dist. AT\nMax margin 0.840 0.846 0.856 0.802 0.792 0.814 0.909 0.901 0.911\nEntropy 0.844 0.839 0.857 0.807 0.798 0.816 0.918 0.912 0.920\nDistance (1) 0.775 0.863 0.862 0.815 0.834 0.866 0.916 0.924 0.926\nDistance (2) 0.876 0.872 0.879 0.833 0.832 0.859 0.9180.929 0.927\nTable 2, legend. Notations are similar to those described in the legend of Table 1, with one distinction: Distance (1) now\ndenotes the regular architecture where the distance score is computed independently for each network in the pair using its\nown embedding, while Distance (2) denotes the hybrid architecture where one network in the pair is ﬁxed to be a Distance\nnetwork, and its embedding is used to compute the distance score for the prediction of the second network in the pair.\nwell. In general, the results reported below are not\nsensitive to the speciﬁc values of the hyper-parameters\nas listed above; we observed only minor changes when\nchanging the values of k;\u000band the margin m.\nMC-Dropout: asproposedinGal&Ghahramani(2015),\nwe used MC dropout in the following manner. We\ntrained each network as usual, but computed the pre-\ndictions while using dropout during test. This was\nrepeated 100 times for each test example, and the\naverage activation was delivered as output.\nAdversarial Training: we used (3) following Goodfellow\net al. (2014), ﬁxing \u000f= 0:1in all the experiments.\n3.2 Error Prediction of Multi-class Labels\nWe ﬁrst compare the performance of our conﬁdence\nscore in the binary task of evaluating whether the net-\nwork’s predicted classiﬁcation label is correct or not.\nWhile our results are independent of the actual accu-\nracy, we note that the accuracy is comparable to those\nachieved with ResNets when not using augmentation\nfor CIFAR-100 or when using only the regular training\ndata for SVHN (see Huang et al. (2016) for example).\nPerformance in this binary task is evaluated using ROC\ncurves computed separately for each conﬁdence score.Results on all three datasets can be seen in Table 1. In\nall cases our proposed distance score, when computed\non a suitably trained network, achieves signiﬁcant im-\nprovement over the alternative scores, even when those\nare enhanced by using either Adversarial Training or\nMC-Dropout.\nTo further test our distance score we evaluate perfor-\nmance over an ensemble of two networks. Results are\nshown in Table 2. Here too, the distance score achieves\nsigniﬁcant improvement over all other methods. We\nalso note that the diﬀerence between the distance score\ncomputedover Distance networks andtheentropyscore\ncomputed over adversarially trained networks, is now\nmuch higher as compared to this diﬀerence when using\nonly one network. As we show in Section 3.3, adversar-\nial training typically leads to a decreased performance\nwhen using an ensemble of networks and relying only\non the entropy score (probably due to a decrease in vari-\nance among the classiﬁers). This observation further\nsupports the added value of our proposed conﬁdence\nscore.\nAs a ﬁnal note, we also used a hybrid architecture using\na matched pair of one classiﬁcation network (of any\nkind) and a second Distance network . The embedding\ndeﬁned by the Distance network is used to compute\nthe distance score for the predictions of the ﬁrst clas-\nsiﬁcation network. Surprisingly, this method achieves\n\nAmit Mandelbaum, Daphna Weinshall\nFigure 1: Accuracy when using an ensemble of networks with CIFAR-100 (top left), STL-10 (top right) and\nSVNH (bottom). The X-axis denotes the number of networks in the ensemble. Absolute accuracy (marked on\nthe leftY-axis) is shown for the 2 most successful ensemble methods among all the methods we had evaluated\n(blue and yellow solid lines, see text), and 2 methods which did not use our distance score including the best\nperforming method in this set (red dotted line, denoted baseline). Diﬀerences in accuracy between the two top\nperformers and the top baseline method are shown using a bar plot (marked on the right Y-axis), with standard\ndeviation of the diﬀerence over (at least) 5 repetitions.\nthe best results in both CIFAR-100 and SVHN while\nbeing comparable to the best result in STL-10. This\nmethod is used later in Section 3.3 to improve accuracy\nwhen running an ensemble of networks. Further inves-\ntigation of this phenomenon lies beyond of the scope\nof the current study.\n3.3 Ensemble Methods\nIn order to evaluate the improvement in performance\nwhen using our conﬁdence score to direct the integra-\ntion of classiﬁers in an ensemble, we used a few common\nways to deﬁne the integration procedure, and a few\nways to construct the ensemble itself. In all compar-\nisons the number of networks in the ensemble remained\nﬁxed atn. Our experiments included the following\nensemble compositions: (a) nregular networks , (b)n\ndistance networks , (c)nAT (Adversarially Trained)\nnetworks , and (d-f) nnetworks such thatn\n2networks\nbelong to one kind of networks ( regular,distance or\nAT) and the remainingn\n2networks belong to another\nkind, spanning all 3 combinations.As described in Section 2.4, the predictions of classiﬁers\nin an ensemble can be integrated using diﬀerent criteria.\nIn general we found that all the methods which did not\nuse our distance score (1), including methods which\nused any of the other conﬁdence score for prediction\nweighting, performed less well than a simple average\nof the softmax activation (method 1 in Section 2.4).\nOtherwise the best performance was obtained when\nusing a weighted average (method 3 in Section 2.4)\nwith weights deﬁned by our distance score (1). With\nvariants (d-f) we also checked two options of obtaining\nthe distance score: (i) Each network deﬁned its own\nconﬁdence score; (ii) in light of the advantage demon-\nstrated by hybrid networks as shown in Section 3.2\nand for each pair of networks from diﬀerent kinds, the\ndistance score for both was computed while using the\nembedding of only one of networks in the pair. MC-\nDropout was not used in this section due to its high\ncomputational cost.\nWhile our experiments included all variants and all\nweighting options, only 4 cases are shown in the fol-\nlowing description of the results in order to improve\n\nDistance-based Conﬁdence Score for Neural Network Classiﬁers\nreadability: 1) the combination achieving best perfor-\nmance; 2) the combination achieving best performance\nwhen not using adversarial training (as ATentails\nadditional computational load at train time); 3) the\nensemble variant achieving best performance without\nusing the distance score (baseline), 4) ensemble average\nwhen using adversarial training without distance score.\nAdditional results for most of the other conditions we\ntested can be found in the supplementary material. To\ngain a better statistical signiﬁcance, each experiment\nwas repeated at least 5 times, with no overlap between\nthe networks.\nCIFAR-100 and STL-10: Fig. 1 shows the ensem-\nble accuracy for the methods mentioned above when\nusing these datasets. It can be clearly seen that weight-\ning predictions based on the distance score from (1)\nimproves results signiﬁcantly. The best results are\nachieved when combining Distance Networks andAd-\nversarial Networks , with signiﬁcant improvement over\nan ensemble of only one kind of networks (not shown\nin the graph). Still, we note importantly that the dis-\ntance score is used to weight bothkind of networks.\nSince Adversarial Training is not always applicable\ndue to its computational cost at train time, we show\nthat the combination of Distance networks andRegular\nnetworks can also lead to signiﬁcant improvement in\nperformance when using the distance score and the\nhybrid architecture described in Section 3.2. Finally\nwe note that Adversarial networks alone achieve very\npoor results when using the original ensemble aver-\nage, further demonstrating the value of the distance\nscore in improving the performance of an ensemble of\nAdversarial networks alone.\nSVHN: Results for this dataset are also shown in Fig.1.\nWhile not as signiﬁcant as those in the other datasets\n(partly due to the high initial accuracy), they are still\nconsistent with them, demonstrating again the power\nand robustness of the distance score.\n3.4 Novelty Detection\nFinally, we compare the performance of the diﬀerent\nconﬁdence scores in the task of novelty detection. In\nthis task the conﬁdence score is used to decide another\nbinary classiﬁcation problem: does the test example be-\nlong to the set of classes the networks had been trained\non, or rather to some unknown class? Performance in\nthis binary classiﬁcation task is evaluated using the\ncorresponding ROC curve of each conﬁdence score.\nWeusedtwocontriveddatasetstoevaluateperformance\nin this task, following the experimental construction\nsuggested in Lakshminarayanan et al. (2016). In the\nﬁrst experiment, we trained the network on the STL-10\ndataset, and then tested it on both STL-10 and SVHN\ntest sets. In the second experiment we switched be-tween the datasets (and changed the trained network)\nmaking SVHN the known dataset and STL-10 the\nnovel one. The task requires to discriminate between\nthe known and the novel datasets. For comparison we\ncomputed novelty, as one often does, with a one-class\nSVM classiﬁer while using the same embeddings. Nov-\nelty thus computed showed much poorer performance,\npossibly because this dataset involves many classes\n(one class SVM is typically used with a single class),\nand therefore these results are not included here.\nTable 3: AUC Results for Novelty Detection.\nConﬁde.\nScoreSTL-10/SVHN SVHN/STL-10\nReg. Dist. AT Reg. Dist. AT\nMax\nMargin.808 .849 .860 .912 .922 .985\nEntropy .810 .857 .870 .917 .933 .992\nDistance .798 .870 .901.904 .934 .996\nTable 3, legend. Left: STL-10 (known) and SVHN (novel).\nRight: SVHN (known) and STL-10 (novel).\nResults are shown in Table 3. Adversarial training,\nwhich was designed to handle this sort of challenge,\nis not surprisingly the best performer. Nevertheless,\nwe see that our proposed conﬁdence score improves\nthe results even further, again demonstrating its added\nvalue.\n4 Conclusions\nWe proposed a new conﬁdence score for multi-class\nneural network classiﬁers. The method we proposed\nto compute this score is scalable, simple to implement,\nand can ﬁt any kind of neural network. This method\nis diﬀerent from other commonly used methods as it is\nbased on measuring the point density in the eﬀective\nembedding space of the network, thus providing a more\ncoherent statistical measure for the distribution of the\nnetwork’s predictions.\nWe also showed that suitable embeddings can be\nachieved by using either a distance-based loss or, some-\nwhat unexpectedly, Adversarial Training. We demon-\nstrated the superiority of the new score in a number\nof tasks. Those tasks were evaluated using a number\nof diﬀerent datasets and with task-appropriate net-\nwork architectures. In all tasks our proposed method\nachieved the best results when compared to traditional\nconﬁdence scores.\nReferences\nAreﬁn, Ahmed Shamsul, Riveros, Carlos, Berretta,\nRegina, and Moscato, Pablo. Gpu-fs-k nn: A soft-\n\nAmit Mandelbaum, Daphna Weinshall\nware tool for fast and scalable k nn computation\nusing gpus. PloS one , 7(8):e44000, 2012.\nBauer, Eric and Kohavi, Ron. An empirical comparison\nofvotingclassiﬁcationalgorithms: Bagging, boosting,\nand variants. Machine learning , 36(1-2):105–139,\n1999.\nBudka, Marcin and Gabrys, Bogdan. Density-\npreserving sampling: robust and eﬃcient alternative\nto cross-validation for error estimation. IEEE trans-\nactions on neural networks and learning systems , 24\n(1):22–34, 2013.\nClevert, Djork-Arné, Unterthiner, Thomas, andHochre-\niter, Sepp. Fast and accurate deep network learning\nby exponential linear units (elus). arXiv preprint\narXiv:1511.07289 , 2015.\nCoates, Adam, Lee, Honglak, and Ng, Andrew Y. An\nanalysis of single-layer networks in unsupervised fea-\nture learning. Ann Arbor , 1001(48109):2, 2010.\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and\nFei-Fei, L. ImageNet: A Large-Scale Hierarchical\nImage Database. In CVPR09 , 2009.\nDietterich, Thomas G. Ensemble methods in machine\nlearning. In International workshop on multiple clas-\nsiﬁer systems , pp. 1–15. Springer, 2000.\nDonahue, Jeﬀrey, Anne Hendricks, Lisa, Guadarrama,\nSergio, Rohrbach, Marcus, Venugopalan, Subhashini,\nSaenko, Kate, and Darrell, Trevor. Long-term re-\ncurrent convolutional networks for visual recognition\nand description. In Proceedings of the IEEE confer-\nence on computer vision and pattern recognition , pp.\n2625–2634, 2015.\nGal, Yarin and Ghahramani, Zoubin. Dropout as\na bayesian approximation: Representing model\nuncertainty in deep learning. arXiv preprint\narXiv:1506.02142 , 2, 2015.\nGarcia, Vincent, Debreuve, Eric, and Barlaud, Michel.\nFast k nearest neighbor search using gpu. In Com-\nputer Vision and Pattern Recognition Workshops,\n2008. CVPRW’08. IEEE Computer Society Confer-\nence on, pp. 1–6. IEEE, 2008.\nGoodfellow, Ian J, Shlens, Jonathon, and Szegedy,\nChristian. Explaining and harnessing adversarial\nexamples. arXiv preprint arXiv:1412.6572 , 2014.\nGunadi, Hendra. Comparing nearest neighbor algo-\nrithms in high-dimensional space. 2011.\nHadsell, Raia, Chopra, Sumit, and LeCun, Yann. Di-\nmensionality reduction by learning an invariant map-\nping. In Computer vision and pattern recognition,\n2006 IEEE computer society conference on , volume 2,\npp. 1735–1742. IEEE, 2006.Hart, Peter. The condensed nearest neighbor rule\n(corresp.). IEEE transactions on information theory ,\n14(3):515–516, 1968.\nHe, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, andSun,\nJian. Deep residual learning for image recognition.\nInProceedings of the IEEE conference on computer\nvision and pattern recognition , pp. 770–778, 2016.\nHoﬀer, Elad and Ailon, Nir. Deep metric learning\nusing triplet network. In International Workshop\non Similarity-Based Pattern Recognition , pp. 84–92.\nSpringer, 2015.\nHu, Hexiang, Zhou, Guang-Tong, Deng, Zhiwei, Liao,\nZicheng, and Mori, Greg. Learning structured infer-\nence neural networks with label relations. In Proceed-\nings of the IEEE Conference on Computer Vision\nand Pattern Recognition , pp. 2960–2968, 2016.\nHuang, Gao, Sun, Yu, Liu, Zhuang, Sedra, Daniel, and\nWeinberger, Kilian Q. Deep networks with stochastic\ndepth. In European Conference on Computer Vision ,\npp. 646–661. Springer, 2016.\nHyvönen, Ville, Pitkänen, Teemu, Tasoulis, Sotiris,\nJääsaari, Elias, Tuomainen, Risto, Wang, Liang,\nCorander, Jukka, and Roos, Teemu. Fast k-nn search.\narXiv preprint arXiv:1509.06957 , 2015.\nKrizhevsky, Alex and Hinton, Geoﬀrey. Learning mul-\ntiple layers of features from tiny images. 2009.\nLakshminarayanan, Balaji, Pritzel, Alexander, and\nBlundell, Charles. Simple and scalable predictive\nuncertainty estimation using deep ensembles. arXiv\npreprint arXiv:1612.01474 , 2016.\nLi, Hui, Wang, Xuesong, and Ding, Shifei. Research\nand development of neural network ensembles: a\nsurvey.Artiﬁcial Intelligence Review , pp. 1–25, 2017.\nMacKay, David JC. Bayesian methods for adaptive\nmodels. PhD thesis, California Institute of Technol-\nogy, 1992.\nMarkou, Markos and Singh, Sameer. Novelty detection:\na review—part 2:: neural network based approaches.\n83(12):2499–2521, 2003.\nMiyato, Takeru, Dai, Andrew M, and Goodfellow, Ian.\nAdversarial training methods for semi-supervised\ntext classiﬁcation. arXiv preprint arXiv:1605.07725 ,\n2016.\nNeal, Radford M. Bayesian learning for neural net-\nworks, volume 118. Springer Science & Business\nMedia, 2012.\nNetzer, Yuval, Wang, Tao, Coates, Adam, Bissacco,\nAlessandro, Wu, Bo, and Ng, Andrew Y. Reading\ndigits in natural images with unsupervised feature\nlearning. In NIPS workshop on deep learning and\nunsupervised feature learning , volume 2011, pp. 5,\n2011.\n\nDistance-based Conﬁdence Score for Neural Network Classiﬁers\nPalatucci, Mark, Pomerleau, Dean, Hinton, Geoﬀrey E,\nand Mitchell, Tom M. Zero-shot learning with seman-\ntic output codes. In Advances in neural information\nprocessing systems , pp. 1410–1418, 2009.\nPimentel, Marco AF, Clifton, David A, Clifton, Lei,\nandTarassenko, Lionel. Areviewofnoveltydetection.\nSignal Processing , 99:215–249, 2014.\nSalakhutdinov, Ruslan and Hinton, Geoﬀrey E. Learn-\ning a nonlinear embedding by preserving class neigh-\nbourhood structure. In AISTATS , volume 11, 2007.\nSchroﬀ, Florian, Kalenichenko, Dmitry, and Philbin,\nJames. Facenet: A uniﬁed embedding for face recog-\nnition and clustering. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recog-\nnition, pp. 815–823, 2015.\nSharif Razavian, Ali, Azizpour, Hossein, Sullivan,\nJosephine, and Carlsson, Stefan. Cnn features oﬀ-\nthe-shelf: an astounding baseline for recognition. In\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition Workshops , pp. 806–\n813, 2014.\nSrivastava, Nitish, Hinton, Geoﬀrey E, Krizhevsky,\nAlex, Sutskever, Ilya, and Salakhutdinov, Ruslan.\nDropout: a simple way to prevent neural networks\nfrom overﬁtting. Journal of Machine Learning Re-\nsearch, 15(1):1929–1958, 2014.\nSzegedy, Christian, Zaremba, Wojciech, Sutskever, Ilya,\nBruna, Joan, Erhan, Dumitru, Goodfellow, Ian, and\nFergus, Rob. Intriguingpropertiesofneuralnetworks.\narXiv preprint arXiv:1312.6199 , 2013.\nTadmor, Oren, Rosenwein, Tal, Shalev-Shwartz, Shai,\nWexler, Yonatan, and Shashua, Amnon. Learning\na metric embedding for face recognition using the\nmultibatch method. In Advances In Neural Informa-\ntion Processing Systems , pp. 1388–1389, 2016.\nTibshirani, Robert. A comparison of some error esti-\nmates for neural network models. Neural Computa-\ntion, 8(1):152–163, 1996.\nVinokurov, Nomi and Weinshall, Daphna. Novelty\ndetection in multiclass scenarios with incomplete\nset of class labels. arXiv preprint arXiv:1604.06242 ,\n2016.\nWeston, Jason, Ratle, Frédéric, Mobahi, Hossein, and\nCollobert, Ronan. Deep learning via semi-supervised\nembedding. In Neural Networks: Tricks of the Trade ,\npp. 639–655. Springer, 2012.\nZaragoza, Hugo and d’Alché Buc, Florence. Conﬁdence\nmeasures for neural network classiﬁers. In Proceed-\nings of the Seventh Int. Conf. Information Processing\nand Management of Uncertainty in Knowlegde Based\nSystems, 1998.", "files_in_pdf": []}