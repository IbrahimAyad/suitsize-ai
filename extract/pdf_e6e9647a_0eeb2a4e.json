{"origin_pdf_path": "https://arxiv.org/pdf/2007.11610", "text_in_pdf": "SIZER: A Dataset and Model for Parsing 3D\nClothing and Learning Size Sensitive 3D\nClothing\nGarvita Tiwari1, Bharat Lal Bhatnagar1, Tony Tung2, and Gerard Pons-Moll1\n1MPI for Informatics, Saarland Informatics Campus, Germany\n2Facebook Reality Labs, Sausalito, USA\nfgtiwari,bbhatnag,gpons g@mpi-inf.mpg.de, tony.tung@fb.com\nFig. 1: SIZER dataset of people with clothing size variation. ( Left): 3D Scans of\npeople captured in di\u000berent clothing styles and sizes. (Right ): T-shirt and short\npants for sizes small and large, which are registered to a common template.\nAbstract. While models of 3D clothing learned from real data exist, no\nmethod can predict clothing deformation as a function of garment size.\nIn this paper, we introduce SizerNet to predict 3D clothing conditioned\non human body shape and garment size parameters, and ParserNet to\ninfer garment meshes and shape under clothing with personal details in a\nsingle pass from an input mesh. SizerNet allows to estimate and visualize\nthe dressing e\u000bect of a garment in various sizes, and ParserNet allows\nto edit clothing of an input mesh directly, removing the need for scan\nsegmentation, which is a challenging problem in itself. To learn these\nmodels, we introduce the SIZER dataset of clothing size variation which\nincludes 100 di\u000berent subjects wearing casual clothing items in various\nsizes, totaling to approximately 2000 scans. This dataset includes the\nscans, registrations to the SMPL model, scans segmented in clothing\nparts, garment category and size labels. Our experiments show better\nparsing accuracy and size prediction than baseline methods trained on\nSIZER . The code, model and dataset will be released for research pur-\nposes at: https://virtualhumans.mpi-inf.mpg.de/sizer/ .arXiv:2007.11610v1  [cs.CV]  22 Jul 2020\n\n2 Tiwari et al.\n1 Introduction\nModeling how 3D clothing \fts on the human body as a function of size has\nnumerous applications in 3D content generation (e.g., AR/VR, movie, video\ngames, sport), clothing size recommendation (e.g., e-commerce), computer vision\nfor fashion, and virtual try-on. It is estimated that retailers lose up to $600 billion\neach year due to sales returns as it is currently di\u000ecult to purchase clothing\nonline without knowing how it will \ft [3,2].\nPredicting how clothing \fts as a function of body shape and garment size\nis an extremely challenging task. Clothing interacts with the body in complex\nways, and \ft is a non-linear function of size and body shape. Furthermore,\nclothing \ft di\u000berences with size are subtle , but they can make a di\u000berence when\npurchasing clothing online. Physics based simulation is still the most commonly\nused technique because it generalizes well, but unfortunately, it is di\u000ecult to\nadjust its parameters to achieve a realistic result, and it can be computationally\nexpensive.\nWhile there exist several works that learn how clothing deforms as a function\nof pose [30], or pose and shape [30,43,22,37,34], there are few works modeling how\ngarments drape as a function of size. Recent works learn a space of styles [50,37]\nfrom physics simulations, but their aim is plausibility, and therefore they can\nnot predict how a real garment will deform on a real body.\nWhat is lacking is (1) a 3D dataset of people wearing the same garments in\ndi\u000berent sizes and (2) a data-driven model learned from real scans which varies\nwith sizing and body shape. In this paper, we introduce the SIZER dataset, the\n\frst dataset of scans of people in di\u000berent garment sizes featuring approximately\n2000 scans, 100 subjects and 10 garments worn by subjects in four di\u000berent sizes.\nUsing the SIZER dataset we learned a Neural Network model, which we refer\nto as SizerNet , which given a body shape and a garment, can predict how the\ngarment drapes on the body as a function of size. Learning SizerNet requires\nto map scans to a registered multi-layer meshes { separate meshes for body\nshape, and top and bottom garments. This requires segmenting the 3D scans,\nand estimating their body shape under clothing, and registering the garments\nacross the dataset, which we obtain using the method explained in [14,38]. From\nthe multi-layer meshes, we learn an encoder to map the input mesh to a latent\ncode, and a decoder which additionally takes the body shape parameters of\nSMPL [33], the size label (S, M, L, XL) of the input garment, and the desired\nsize of the output, to predict the output garment as a displacement \feld to a\ntemplate.\nAlthough visualizing how an existing garment \fts on a body as a function of\nsize is already useful for virtual try-on applications, we would also like to change\nthe size of garments in existing 3D scans. Scans however, are just pointclouds,\nand parsing them into a multi-layer representation at test time using [14,38]\nrequires segmentation, which sometimes requires manual intervention. There-\nfore, we propose ParserNet , which automatically maps a single mesh registration\n(SMPL deformed to the scan) to multi-layer meshes with a single feed-forward\npass. ParserNet , not only segments the single mesh registration, but it reparam-\n\nSIZER 3\neterizes the surface so that it is coherent with common garment templates. The\noutput multi-layer representation of ParserNet is powerful as it allows simula-\ntion and editing meshes separately. Additionally, the tandem of SizerNet and\nParserNet allows us to edit the size of clothing directly on the mesh, allowing\nshape manipulation applications never explored before.\nIn summary, our contributions are:\n\u000fSIZER dataset: A dataset of clothing size variation of approximately 2000\nscans including 100 subjects wearing 10 garment classes in di\u000berent sizes,\nwhere we make available, scans, clothing segmentation, SMPL+G registra-\ntions, body shape under clothing, garment class and size labels.\n\u000fSizerNet: The \frst model learned from real scans to predict how clothing\ndrapes on the body as a function of size.\n\u000fParserNet: A data-driven model to map a single mesh registration into a\nmulti-layered representation of clothing without the need for segmentation\nor non-linear optimization.\nFig. 2: We propose a model to estimate and visualize the dressing e\u000bect of a gar-\nment conditioned on body shape and garment size parameters. For this we intro-\nduce ParserNet (fU\nw;fL\nw;fB\nw), which takes a SMPL registered mesh M(\u0012;\f;D)\nas input and predicts the SMPL parameters ( \u0012;\f), parsed 3D garments using\nprede\fned templates Tg(\f;\u0012;0) and predicts body shape under clothing while\npreserving the personal details of the subject. We also propose SizerNet , an\nencoder-decoder ( fenc\nw;fdec\nw) based network, that resizes the garment given as\ninput with the desired size label ( \u000ein;\u000eout) and drapes it on the body shape\nunder clothing.\n2 Related Work\nClothing modeling. Accurate reconstruction of 3D cloth with \fne structures\n(e.g., wrinkles) is essential for realism while being notoriously challenging. Meth-\nods based on multi-view stereo can recover global shape robustly but struggle\nwith high frequency details in non-textured regions [51,44,16,6,47,32]. The pio-\nneering work of [9,8] demonstrated for the \frst time detailed body and clothing\n\n4 Tiwari et al.\nreconstruction from monocular video using a displacement from SMPL, which\nspearheaded recent developments [23,7,10,42,24,25]. These approaches do not\nseparate body from clothing. In [38,30,14,26], the authors propose to recon-\nstruct clothing as a layer separated from the body. These models are trained\non 3D scans of real clothed people data and produce realistic models. On the\nother hand, physics based simulation methods have also been used to model\nclothing [48,49,35,21,45,46,37,43,22]. Despite the potential gap with real-world\ndata, they are a great alternative to obtain clean data, free of acquisition noise\nand holes. However, they still require manual parameter tuning (e.g., time step\nfor better convergence, sheer and stretch for better deformation e\u000bects, etc.),\nand can be slow or unstable. In [43,22,21] a pose and shape dependent clothing\nmodel is introduced, and [37,50] also model garment style dependent clothing\nusing a lower-dimensional representation for style and size like PCA and garment\nsewing parameters, however there is no direct control on the size of clothing gen-\nerated for given body shape. In [53], authors model the garment \ft on di\u000berent\nbody shapes from images. Our model SizerNet automatically outputs realistic\n3D cloth models conditioned on desired features (e.g., shape, size).\nShape under clothing. In [11,60,57], the authors propose to estimate body\nshape under clothing by \ftting a 3D body model to 3D reconstructions of people.\nAn objective function typically forces the body to be inside clothing while be-\ning close to the skin region. These methods cannot generalize well to complex or\nloose clothing without additional prior or supervision [17]. In [27,36,54,29,28,52],\nthe authors propose learned models to estimate body shape from 2D images of\nclothed people, but shape accuracy is limited due to depth ambiguity. Our model\nParserNet takes as input a 3D mesh and outputs 3D bodies under clothing with\nhigh \fdelity while preserving subject identity (e.g., face details).\nCloth parsing. The literature has proposed several methods for clothed hu-\nman understanding. In particular, e\u000ecient cloth parsing in 2D has been achieved\nusing supervised learning and generative networks [55,56,58,18,19,20]. 3D cloth-\ning parsing of 3D scans has also been investigated [38,14]. The authors propose\ntechniques based on MRF-GrabCut [41] to segment 3D clothing from 3D scans\nand transfer them to di\u000berent subjects. However the approach requires several\nsteps, which is not optimal for scalability. We extend previous work with SIZER ,\na fully automatic data-driven pipeline. In [13], the authors jointly predict cloth-\ning and inner body surface, with semantic correspondences to SMPL. However,\nit does not have semantic clothing information.\n3D datasets. To date, only a few datasets consist of 3D models of subjects\nwith segmented clothes. 3DPeople [40], Cloth3D [12] consists of a large dataset\nof synthetic 3D humans with clothing. None of the synthetic datasets contains\nrealistic cloth deformations like the SIZER dataset. THUman [61] consists of\nsequences of clothed 3D humans in motion, captured with a consumer RGBD\nsensor (Kinectv2), and are reconstructed using volumetric SDF fusion [59]. How-\n\nSIZER 5\never, 3D models are rather smooth compared to our 3D scans and no ground\ntruth segmentation of clothing is provided. Dyna and D-FAUST [39,15] consist of\nhigh-res 3D scans of 10 humans in motion with di\u000berent shape but the subjects\nare only wearing minimal clothing. BUFF [60] contains high-quality 3D scans of\n6 subjects with and without clothing. The dataset is primarily designed to train\nmodels to estimate body shape under clothing and doesn't contain garments seg-\nmentation. In [14], the authors create a digital wardrobe with 3D templates of\ngarments to dress 3D bodies. In [26], authors propose a mixture of synthetic and\nreal data, which contains garment, body shape and pose variations. However,\nthe fraction of real dataset ( \u0018300 scans) is fairly small. DeepFahsion3D [62] is a\ndataset of real scans of clothing containing various garment styles. None of these\ndatasets contain garment sizing variation. Unlike our proposed SIZER dataset,\nno dataset contains a large amount of pre-segmented clothing from 3D scans at\ndi\u000berent sizes, with corresponding body shapes under clothing.\n3 Dataset\nIn this paper, we address a very challenging problem of modeling garment \ftting\nas a function of body shape and garment size. As explained in Sec. 2, one of the\nkey bottlenecks that hinder progress in this direction is the lack of real-world\ndatasets that contain calibrated and well-annotated garments in di\u000berent sizes\ndraped on real humans. To this end, we present SIZER dataset, a dataset of over\n2000 scans containing people in diverse body shapes in various garments styles\nand sizes. We describe our dataset in Sec. 3.1 and 3.2.\n3.1 SIZER dataset: Scans\nWe introduce the SIZER dataset that contains 100 subjects, wearing the same\ngarment in 2 or 3 garment sizes (S, M, L, XL). We include 10 garment classes,\nnamely shirt, dress-shirt, jeans, hoodie, polo t-shirt, t-shirt, shorts, vest, skirt,\nand coat, which amounts to roughly 200 scans per garment class. We capture\nthe subjects in a relaxed A-pose to avoid stretching or tension due to pose in\nthe garments. Figure 1 shows some examples of people wearing a \fxed set of\ngarments in di\u000berent sizes. We use a Treedy's static scanner [5] which has 130+\ncameras, and reconstruct the scans using Agisoft's Metashape software [1]. Our\nscans are high resolution and are represented by meshes, which have di\u000berent\nunderlying graph connectivity across the dataset, and hence it is challenging to\nuse this dataset directly in any learning framework. We preprocess our dataset,\nby registering them to SMPL [33]. We explain the structure of processed data\nin the following section.\n3.2 SIZER dataset: SMPL and Garment registrations\nTo improve general usability of the SIZER dataset, we provide SMPL+G reg-\nistrations [31,14] registrations. Registering our scans to SMPL, brings all our\n\n6 Tiwari et al.\nscans to correspondence, and provides more control over the data via pose and\nshape parameters from the underlying SMPL. We brie\ry describe the SMPL\nand SMPL+G formulations below.\nSMPL represents the human body as a parametric function M(\u0001), of pose (\u0012)\nand shape (\f). We add per-vertex displacements ( D) on top of SMPL to model\ndeformations corresponding to hair, garments, etc. thus resulting in the SMPL\nmodel. SMPL applies standard skinning W(\u0001) to a base template Tin T-pose.\nHere, Wdenotes the blend weights and Bp(\u0001) andBs(\u0001) models pose and shape\ndependent deformations respectively.\nM(\f;\u0012;D) =W(T(\f;\u0012;D);J(\f);\u0012;W) (1)\nT(\f;\u0012;D) =T+Bs(\f) +Bp(\u0012) +D (2)\nSMPL+G is a parametric formulation to represent the human body and\ngarments as separate meshes.To register the garments we \frst segment scans into\ngarments and skin parts [14]. We re\fne the scan segmentation step used in [14] by\n\fne-tuning the Human Parsing network [20] with a multi-view consistency loss.\nWe then use the multi-mesh registration approach from [14] to register garments\nto the SMPL+G model. For each garment class, we obtain a template mesh which\nis de\fned as a subset of the SMPL template, given by Tg(\f;\u0012;0) =IgT(\f;\u0012;0),\nwhere Ig2Zmg\u0002n\n2 is an indicator matrix, with Ig\ni;j= 1 if garment gvertex\ni2f1:::m ggis associated with body shape vertex j2f1:::ng.mgandn\ndenote the number of vertices in the garment template and the SMPL mesh\nrespectively. Similarly, we de\fne a garment function G(\f;\u0012;Dg) using Eq. (3),\nwhere Dgare the per-vertex o\u000bsets from the template\nG(\f;\u0012;Dg) =W(Tg(\f;\u0012;Dg);J(\f);\u0012;W): (3)\nFor every scan in the SIZER dataset, we will release the scan, segmented\nscan, and SMPL+G registrations, garment category and garment size label.\nThis dataset can be used in several applications like virtual try-on, character\nanimation, learning generative models, data-driven body shape under clothing,\nsize and(or) shape sensitive clothing model, etc. To stimulate further research\nin this direction, we will release the dataset,code and baseline models, which\ncan be used as a benchmark in 3D clothing parsing and 3D garment resizing.\nWe use this dataset to build a model for the task of garment extraction from\nsingle mesh ( ParserNet ) and garment resizing ( SizerNet ), which we describe in\nthe next section.\n4 Method\nWe introduce ParserNet (Sec. 4.2), the \frst method for extracting garments\ndirectly from SMPL registered meshes. For parsing garments, we \frst predict the\nunderlying body SMPL parameters using a pose and shape prediction network\n(Sec. 4.1) and use ParserNet to extract garment layers and personal features\n\nSIZER 7\nlike hair, facial features to create body shape under clothing. Next, we present\nSizerNet (Sec. 4.3), an encoder-decoder based deep network for garment resizing.\nAn overview of the method is shown in Fig. 2.\n4.1 Pose and shape prediction network\nTo estimate body shape under clothing, we \frst create the undressed SMPL\nbody for a given clothed input single layer mesh M(\f;\u0012;D), by predicting \u0012;\f\nusingf\u0012\nwandf\f\nwrespectively. We train f\u0012\nwandf\f\nwwithL2loss over parameters\nand per-vertex loss between predicted SMPL body and clothed input mesh, as\nshown in Eq. (4) and (5). Since the reference body under clothing parameters\n\u0012;\fobtained via instance speci\fc optimization (Sec. 3.2) can be inaccurate, we\nadd an additional per-vertex loss between our predicted SMPL body vertices\nM(^\u0012;^\f;0) and the input clothed mesh M(\f;\u0012;D). This brings the predicted\nundressed body closer to the input clothed mesh. We observe more stable results\ntrainingf\u0012\nwandf\f\nwseparately initially, using the reference \fand\u0012respectively.\nSince the\fcomponents in SMPL are normalized to have \u001b= 1, we un-normalize\nthem by scaling by their respective standard deviations [ \u001b1;\u001b2;:::;\u001b 10] as given\nin Eq. (5).\nL\u0012=wposejj^\u0012\u0000\u0012jj2\n2+wvjjM(\f;^\u0012;0)\u0000M(\f;\u0012;D)jj (4)\nL\f=wshape10X\ni=1\u001bi(^\fi\u0000\fi)2+wvjjM(^\f;\u0012;0)\u0000M(\f;\u0012;D)jj (5)\nHere,wpose,wshape andwvare weights for the loss on pose, shape and pre-\ndicted SMPL surface. ( ^\u0012;^\f) denote predicted parameters. The output is a smooth\n(SMPL model) body shape under clothing.\n4.2 ParserNet\nParsing garments. Parsing garments from a single mesh ( M) can be done by\nsegmenting it into separate garments for each class ( Gg;k\nseg), which leads to di\u000ber-\nent underlying graph connectivity ( Gg;k\nseg= (Gg;k\nseg;Eg;k\nseg)) across all the instances\n(k) of a garment class g, shown in Fig. 3 (right). Hence, we propose to parse\ngarments by deforming vertices of a template Tg(\f;\u0012;0) with \fxed connectivity\nEg, obtaining vertices Gg;k2Gg;k, whereGg;k= (Gg;k;Eg), shown in Fig. 3\n(middle).\nOur key idea is to predict the deformed vertices Ggdirectly as a convex\ncombination of vertices of the input mesh M=M(\f;\u0012;D) with a learned\nsparse regressor matrix Wg, such that Gg=WgM. Speci\fcally, ParserNet\npredicts the sparse matrix ( Wg) as a function of input mesh features (vertices\nand normals) and a prede\fned per-vertex neighborhood ( Ni) for every vertex i\nof garment class g. We will henceforth drop ( :)g;kunless required. In this way,\n\n8 Tiwari et al.\nFig. 3: Left to right: Input single mesh ( Mk), garment template ( Tg(\f;\u0012;0) =\nIgT(\f;\u0012;0)), garment mesh extracted using Gg;k=IgMk, multi-layer meshes\n(Gg;k) registered to SMPL+G, all with garment class speci\fc edge connectivity\nEg, and segmented scan Gg;k\nsegwith instance speci\fc edge connectivity Eg;k\nseg.\nthe output vertices Gi2R3, wherei2f1;:::;m gg, are obtained as a convex\ncombination of input mesh vertices Mj2R3in a prede\fned neighborhood ( Ni).\nGi=X\nj2NiWijMj: (6)\nParsing detailed body shape under clothing. For generating detailed body\nshape under clothing, we \frst create a smooth body mesh , using SMPL param-\neters\u0012and\fpredicted from f\u0012\nw;f\f\nw(Sec. 4.1). Using the same aforementioned\nconvex combination formulation, Body ParserNet transfers the visible skin ver-\ntices from the input mesh to the smooth body mesh, obtaining hair and fa-\ncial features. We parse the input mesh into upper, lower garments and detailed\nshape under clothing using 3 sub-networks ( fU\nw;fL\nw;fB\nw) ofParserNet , as shown\nin Fig. 2.\n4.3 SizerNet\nWe aim to edit the garment mesh based on garment size labels such as S, M,\nL, etc, to see the dressing e\u000bect of the garment for a new size. For this task,\nwe propose an encoder-decoder based network, which is shown in Fig. 2 (right).\nThe network fenc\nw, encodes the garment mesh Ginto a lower-dimensional latent\ncodexgar2Rd, shown in Eq. (7). We append ( \f;\u000ein;\u000eout) to the latent space,\nwhere\u000ein;\u000eoutare one-hot encodings of input and desired output sizing and \f\nis the SMPL \fparameter for underlying body shape.\nxgar=fenc\nw(Gin); fenc\nw(:) :Rmg\u00023!Rd(7)\nThe decoder network, fdec\nw(:) :Rj\fj\u0002Rd\u0002R2j\u000ej!Rmg\u00023predicts the dis-\nplacement \feld Dg=fdec\nw(\f;xgar;\u000ein;\u000eout) on top on template. We obtain the\noutput garment Goutin the new desired size \u000eoutusing Eq. (3).\n\nSIZER 9\n4.4 Loss functions\nWe train the networks, ParserNet and SizerNet with training losses given by\nEq. (8) and (9) respectively, where w3D; wnorm; wlap; winterp andwware weights\nfor the loss on vertices, normal, Laplacian, interpenetration and weight regular-\nizer term respectively. We explain each of the loss terms in this section.\nLparser =w3DL3D+wnormLnorm+wlapLlap+winterpLinterp +wwLw (8)\nLsizer=w3DL3D+wnormLnorm+wlapLlap+winterpLinterp (9)\n\u000f3D vertex loss for garments. We de\fneL3DasL1loss between predicted\nand ground truth vertices\nL3D=jjGP\u0000GGTjj1: (10)\n\u000f3D vertex loss for shape under clothing. For training fB\nw(ParserNet\nfor the body), we use the input mesh skin as supervision for predicting per-\nsonal details of subject. We de\fne a garment class speci\fc geodesic distance\nweighted loss term, as shown in Eq. (11), where Isis the indicator matrix\nfor skin region and wgeois a vector containing the sigmoid of the geodesic\ndistances from vertices to the boundary between skin and non-skin regions.\nThe loss term is high when the prediction is far from the input mesh Mfor\nthe visible skin region, and lower for the cloth region, with a smooth tran-\nsition regulated by the geodesic term. Let abs ij(\u0001) denote an element-wise\nabsolute value operator. Then the loss is computed as\nLbody\n3D=kwT\ngeo\u0001absij(Gs\nP\u0000IsM)k1: (11)\n\u000fNormal Loss. We de\fneLnorm as the di\u000berence in angle between ground\ntruth face normal ( Ni\nGT) and predicted face normal ( Ni\nP).\nLnorm =1\nNfacesNfacesX\ni(1\u0000(NGT;i)TNP;i): (12)\n\u000fLaplacian smoothness term. This enforces the Laplacian of predicted\ngarment mesh to be close to the Laplacian of ground truth mesh. Let Lg2\nRmg\u0002mgbe the graph Laplacian of the garment mesh GGT, and\u0001init=\nLgGGT2Rmg\u00023be the di\u000berential coordinates of the GGT, then we com-\npute the Laplacian smoothness term for a predicted mesh GPas\nLlap=jj\u0001init\u0000LgGPjj2: (13)\n\u000fInterpenetration loss. Since minimizing per-vertex loss does not guar-\nantee that the predicted garment lies outside the body surface, we use the\ninterpenetration loss term in Eq. (14) proposed in GarNet [22]. For every\nvertex GP;j, we \fnd the nearest vertex in the predicted body shape under\nclothing ( Bi) and de\fne the body-garment correspondences as C(B;GP).\n\n10 Tiwari et al.\nLetNibe the normal of the ithbody vertex Bi. If the predicted garment\nvertex GP;jpenetrates the body, it is penalized with the following loss\nLinterp =X\n(i;j)2C(B;GP)1d(GP;j;GGT;j)<dtolReLU (\u0000Ni(GP;j\u0000Bi))=mg;(14)\nwhere notice that 1d(GP;j;GGT;j)<dtolactivates the loss when the distance\nbetween predicted garment mesh vertices and ground truth mesh vertices is\nsmall i.e.<dtol.\n\u000fWeight regularizer. To preserve the \fne details when parsing the input\nmesh, we want the weights predicted by the network to be sparse and con-\n\fned in a local neighborhood. Hence, we add a regularizer which penalizes\nlarge values for Wijif the distance between of Mjand the vertex Mkwith\nlargest weight k= arg maxjWijis large. Let d(\u0001;\u0001) dennote Euclidean dis-\ntance between vertices, then the regularizer equals\nLw=mgX\ni=1X\nj2NiWijd(Mk;Mj); k= arg maxjWij: (15)\n4.5 Implementation Details\nWe implement f\u0012\nwandf\f\nwnetworks with 2 fully connected and a linear output\nlayer. We implement ParserNet fU\nw;fL\nw;fB\nwwith 3 fully connected layers. We\nuse neighborhood ( Ni) size ofjNij= 50, for our experiments. We \frst train\nthe network for garment classes which share the same garment template and\nthen \fne-tune separately for each garment class g. To speed up training for\nParserNet , we train the network to predict Wg=Ig, where Igis the indicator\nmatrix for garment class g, explained in Sec. 3.2. This initializes the network to\nparse the garment by cutting out a part of the input mesh based on the constant\nper-garment indicator matrix, shown in Fig. 3.\nForSizerNet we used= 30 and we implement fenc\nw;fdec\nwwith fully connected\nlayers and skip connections between encoder and decoder network. We held out\n40 scans for testing in each garment class, which includes some cases with unseen\nsubjects and some with unseen garment size for seen subjects. For pose-shape\nprediction network, ParserNet and SizerNet we use batch-size of 8 and learning\nrate of 0:0001.\n5 Experiments and Results\n5.1 Results of 3D garment parsing and shape under clothing\nTo validate the choice of parsing the garments using a sparse regressor matrix\n(W), we compare the results of ParserNet with two baseline approaches: 1) A\nlinearized version of ParserNet implemented with LASSO, and 2) A naive FC\n\nSIZER 11\nFig. 4: Comparison of ParserNet with a FC network from front and lateral view.\nnetwork, which has the same architecture as ParserNet . However, instead of pre-\ndicting the weight matrix ( W), the FC network directly predicts the deformation\n(Dg) from the garment template ( Tg(\f;\u0012;0)) for a given input ( M).\nWe compare the per-vertex error of ParserNet with the aforementioned base-\nlines in Tab. 1. Figure 4 shows that ParserNet can produce details, \fne wrinkles,\nand large garment deformations, which is not possible with a naive FC network.\nThis is attainable because ParserNet reconstructs the output garment mesh as\na localized sparse weighted sum of input vertex locations, and hence preserves\nthe geometry details present in the input mesh. However, in the case of naive FC\nnetwork, the predicted displacement \feld ( Dg) is smooth and does not explain\nlarge deformations. Hence, naive FC network is not able to predict loose gar-\nments and does not preserve \fne details. We show results of ParserNet for more\ngarment classes in Fig. 5 and add more results in the supplementary material.\n5.2 Results of garment resizing\nEditing garment meshes based on garment size label is an unexplored problem\nand, hence there are no well de\fned quantitative metrics. We introduce two\nquantitative metrics, namely change in mesh surface area (Aerr) and per-vertex\nerror (Verr) for evaluating the resizing task. Surface area accounts for the scale\nof a garment, which only changes with the garment size, and per-vertex error\naccounts for details and folds created due to the underlying body shape and\nlooseness/tightness of the garment. Moreover, subtle changes in garment shape\n\n12 Tiwari et al.\nGarment Linear\nModelFC ParserNet Garment Linear\nModelFC ParserNet\nPolo 32.21 17.25 14.33 Shorts 29.78 20.12 16.07\nShirt 27.63 19.35 14.56 Pants 34.82 18.2 17.24\nVest 28.17 18.56 15.89 Coat 41.27 22.19 15.34\nHoodies 37.34 23.69 15.76 Shorts2 31.38 23.45 16.23\nT-Shirt 26.94 15.98 13.77\nTable 1: Average per-vertex error Verrof proposed method for parsing garment\nmeshes for di\u000berent garment class (in mm).\nFig. 5: Input single mesh and ParserNet results for more garments.\nwith respect to size are di\u000ecult to evaluate. Hence, we use heat map visualiza-\ntions for qualitative analysis of the results.\nSince there is no other existing work for garment resizing task to compare\nwith, we evaluate our method against the following three baselines.\n1.Error margin in data: We de\fne error margin as the change in per-vertex\nlocation (Verr) and surface area (Aerr) between garments of two consecutive\nsize for a subject in the dataset. Our model should ideally produce a smaller\nerror than this margin.\n2.Average prediction : For every subject in the dataset, we create the average\ngarment (Gavg), by averaging over all the available sizes for a subject.\n3.Linear scaling + Alignment : We linearly scale the garment mesh, according\nto desired size label, and then align the garment to the underlying body.\nTable 2 shows the errors for each experiment. SizerNet results in lower errors,\nas compared to the linear scaling method, which re\rects the need for modelling\n\nSIZER 13\nFig. 6: (a) Input single mesh. (b) Parsed multi-layer mesh from ParserNet. (c),(d)\nResized garment in two subsequent smaller sizes. (e), (f) Heatmap of change in\nper vertex error on original parsed garment for two new sizes.\nthe non-linear relationship between garment shape, underlying body shape and\ngarment size. We also see that network predictions yield lower error as compared\nto average garment prediction, which suggests that the model is learning the size\nvariation, even though the di\u000berences in the ground truth itself are subtle. We\npresent the results of SizerNet for common garment classes in Tab. 2, Fig. 6, 7\nand add more results in the supplementary material.\nGarment Error-margin Average-pred Linear Scaling Ours\nVerr Aerr Verr Aerr Verr Aerr Verr Aerr\nPolo t-shirt 33.25 24.56 23.86 3.63 35.05 8.45 16.42 1.79\nShirt 36.52 19.57 21.95 2.76 34.53 7.01 15.54 1.41\nShorts 43.21 27.21 24.79 5.41 35.77 4.99 16.71 2.38\nPants 30.83 15.15 21.54 4.73 38.16 7.13 19.26 2.43\nTable 2: Average per vertex error ( Verrinmm) and surface area error( Aerrin\n%) of proposed method for garment resizing.\n\n14 Tiwari et al.\n(a) Small(input, parsed), Medium, Large\n (b) Medium(input, parsed), Small, Large\n(c) Large(input, parsed), Medium, XLarge\n (d) XLarge(input,parse), Large, Medium\nFig. 7: Results of ParserNet +SizerNet , where we parse the garments from input\nsingle mesh and change the size of garment to visualise dressing e\u000bect.\n6 Conclusion\nWe introduce SIZER , a clothing size variation dataset and model, which is the\n\frst real dataset to capture clothing size variation on di\u000berent subjects. We\nalso introduce ParserNet : a 3D garment parsing network and SizerNet : a size\nsensitive clothing model. With this method, one can change the single mesh\nregistration to multi-layer meshes of garments and body shape under clothing,\nwithout the need for scan segmentation and can use the result for animation,\nvirtual try-on, etc. SizerNet can drape a person with garments in di\u000berent sizes.\nSince our dataset only consists of roughly aligned A-poses, we are limited to\nA-pose. We only exploit geometry information (vertices and normals) for 3D\nclothing parsing. In future work, we plan to use the color information in Parser-\nNetvia texture augmentation, to improve the accuracy and generalization of\nthe proposed method. We will release the model, dataset, and code to stimu-\nlate research in the direction of 3D garment parsing, segmentation, resizing and\npredicting body shape under clothing.\nAcknowledgements. This work is funded by the Deutsche Forschungsgemein-\nschaft (DFG, German Research Foundation) - 409792180 (Emmy Noether Programme,\nproject: Real Virtual Humans) and a Facebook research award. We thank Tarun,\nNavami, and Yash for helping us with the data capture and RVH team members [4],\nfor their meticulous feedback on this manuscript.\n\nSIZER 15\nReferences\n1. Agisoft metashape, https://www.agisoft.com/\n2. The high cost of retail returns, https://www.thebalancesmb.com/\nthe-high-cost-of-retail-returns-2890350\n3. Ihl group, https://www.ihlservices.com/\n4. Real virtual humans, max planck institute for informatics, https://\nvirtualhumans.mpi-inf.mpg.de/people.html\n5. Treedy's scanner, https://www.treedys.com\n6. de Aguiar, E., Stoll, C., Theobalt, C., Ahmed, N., Seidel, H., Thrun, S.: Perfor-\nmance capture from sparse multi-view video. ACM Trans. Graph. 27(3), 98:1{98:10\n(2008)\n7. Alldieck, T., Magnor, M., Bhatnagar, B.L., Theobalt, C., Pons-Moll, G.: Learning\nto reconstruct people in clothing from a single RGB camera. In: IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) (jun 2019)\n8. Alldieck, T., Magnor, M., Xu, W., Theobalt, C., Pons-Moll, G.: Detailed human\navatars from monocular video. In: International Conference on 3D Vision (3DV)\n(sep 2018)\n9. Alldieck, T., Magnor, M., Xu, W., Theobalt, C., Pons-Moll, G.: Video based re-\nconstruction of 3d people models. In: IEEE Conference on Computer Vision and\nPattern Recognition (CVPR) (June 2018)\n10. Alldieck, T., Pons-Moll, G., Theobalt, C., Magnor, M.: Tex2shape: Detailed full\nhuman body geometry from a single image. In: IEEE International Conference on\nComputer Vision (ICCV). IEEE (oct 2019)\n11. B\u0015 alan, A.O., Black, M.J.: The naked truth: Estimating body shape under clothing.\nIn: European Conf. on Computer Vision. pp. 15{29. Springer (2008)\n12. Bertiche, H., Madadi, M., Escalera, S.: CLOTH3D: clothed 3d humans. vol.\nabs/1912.02792 (2019)\n13. Bhatnagar, B.L., Sminchisescu, C., Theobalt, C., Pons-Moll, G.: Combining im-\nplicit function learning and parametric models for 3d human reconstruction. In:\nEuropean Conference on Computer Vision (ECCV). Springer (August 2020)\n14. Bhatnagar, B.L., Tiwari, G., Theobalt, C., Pons-Moll, G.: Multi-garment net:\nLearning to dress 3d people from images. In: IEEE International Conference on\nComputer Vision (ICCV). IEEE (oct 2019)\n15. Bogo, F., Romero, J., Pons-Moll, G., Black, M.J.: Dynamic FAUST: Registering\nhuman bodies in motion. In: IEEE Conf. on Computer Vision and Pattern Recog-\nnition (2017)\n16. Bradley, D., Popa, T., She\u000ber, A., Heidrich, W., Boubekeur, T.: Markerless garment\ncapture. In: ACM Transactions on Graphics. vol. 27, p. 99. ACM (2008)\n17. Chen, X., Pang, A., Zhu, Y., Li, Y., Luo, X., Zhang, G., Wang, P., Zhang, Y., Li, S.,\nYu, J.: Towards 3d human shape recovery under clothing. CoRR abs/1904.02601\n(2019)\n18. Dong, H., Liang, X., Wang, B., Lai, H., Zhu, J., Yin, J.: Towards multi-pose guided\nvirtual try-on network. International Conference on Computer Vision (ICCV)\n(2019)\n19. Dong, H., Liang, X., Zhang, Y., Zhang, X., Xie, Z., Wu, B., Zhang, Z., Shen, X.,\nYin, J.: Fashion editing with adversarial parsing learning. Conference on Computer\nVision and Pattern Recognition (CVPR) (2020)\n20. Gong, K., Liang, X., Li, Y., Chen, Y., Yang, M., Lin, L.: Instance-level human\nparsing via part grouping network. In: ECCV (2018)\n\n16 Tiwari et al.\n21. Guan, P., Reiss, L., Hirshberg, D., Weiss, A., Black, M.J.: DRAPE: DRessing\nAny PErson. ACM Trans. on Graphics (Proc. SIGGRAPH) 31(4), 35:1{35:10 (Jul\n2012)\n22. Gundogdu, E., Constantin, V., Seifoddini, A., Dang, M., Salzmann, M., Fua, P.:\nGarnet: A two-stream network for fast and accurate 3d cloth draping. In: IEEE\nInternational Conference on Computer Vision (ICCV). IEEE (oct 2019)\n23. Habermann, M., Xu, W., , Zollhoefer, M., Pons-Moll, G., Theobalt, C.: Livecap:\nReal-time human performance capture from monocular video (oct 2019)\n24. Habermann, M., Xu, W., , Zollhoefer, M., Pons-Moll, G., Theobalt, C.: Deepcap:\nMonocular human performance capture using weak supervision. In: IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR). IEEE (jun 2020)\n25. Huang, Z., Xu, Y., Lassner, C., Li, H., Tung, T.: Arch: Animatable reconstruction\nof clothed humans. In: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition. pp. 3093{3102 (2020)\n26. Jiang, B., Zhang, J., Hong, Y., Luo, J., Liu, L., Bao, H.: Bcnet: Learning body\nand cloth shape from a single image. arXiv preprint arXiv:2004.00214 (2020)\n27. Kanazawa, A., Black, M.J., Jacobs, D.W., Malik, J.: End-to-end recovery of human\nshape and pose. In: Computer Vision and Pattern Regognition (CVPR) (2018)\n28. Kolotouros, N., Pavlakos, G., Black, M.J., Daniilidis, K.: Learning to reconstruct\n3D human pose and shape via model-\ftting in the loop. In: International Confer-\nence on Computer Vision (Oct 2019)\n29. Kolotouros, N., Pavlakos, G., Daniilidis, K.: Convolutional mesh regression for\nsingle-image human shape reconstruction. In: CVPR (2019)\n30. Laehner, Z., Cremers, D., Tung, T.: Deepwrinkles: Accurate and realistic cloth-\ning modeling. In: European Conference on Computer Vision (ECCV) (September\n2018)\n31. Lazova, V., Insafutdinov, E., Pons-Moll, G.: 360-degree textures of people in cloth-\ning from a single image. In: International Conference on 3D Vision (3DV) (sep\n2019)\n32. Leroy, V., Franco, J., Boyer, E.: Multi-view dynamic shape re\fnement using lo-\ncal temporal integration. In: IEEE International Conference on Computer Vision,\nICCV. pp. 3113{3122. Venice, Italy (oct 2017)\n33. Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black, M.J.: SMPL: A\nskinned multi-person linear model. ACM Trans. Graphics (Proc. SIGGRAPH Asia)\n34(6), 248:1{248:16 (Oct 2015)\n34. Ma, Q., Yang, J., Ranjan, A., Pujades, S., Pons-Moll, G., Tang, S., Black, M.:\nLearning to dress 3d people in generative clothing. In: IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR). IEEE (jun 2020)\n35. Miguel, E., Bradley, D., Thomaszewski, B., Bickel, B., Matusik, W., Otaduy, M.A.,\nMarschner, S.: Data-driven estimation of cloth simulation models. Comput. Graph.\nForum 31(2), 519{528 (2012)\n36. Omran, M., Lassner, C., Pons-Moll, G., Gehler, P., Schiele, B.: Neural body \ftting:\nUnifying deep learning and model based human pose and shape estimation. In:\nInternational Conf. on 3D Vision (2018)\n37. Patel, C., Liao, Z., Pons-Moll, G.: The virtual tailor: Predicting clothing in 3d\nas a function of human pose, shape and garment style. In: IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR). IEEE (Jun 2020)\n38. Pons-Moll, G., Pujades, S., Hu, S., Black, M.: ClothCap: Seamless 4D clothing\ncapture and retargeting. ACM Transactions on Graphics 36(4) (2017)\n39. Pons-Moll, G., Romero, J., Mahmood, N., Black, M.J.: Dyna: a model of dynamic\nhuman shape in motion. ACM Transactions on Graphics 34, 120 (2015)\n\nSIZER 17\n40. Pumarola, A., Sanchez, J., Choi, G., Sanfeliu, A., Moreno-Noguer, F.: 3DPeople:\nModeling the Geometry of Dressed Humans. In: International Conference in Com-\nputer Vision (ICCV) (2019)\n41. Rother, C., Kolmogorov, V., Blake, A.: Grabcut: Interactive foreground extraction\nusing iterated graph cuts. vol. 23 (2004)\n42. Saito, S., Huang, Z., Natsume, R., Morishima, S., Kanazawa, A., Li, H.: Pifu:\nPixel-aligned implicit function for high-resolution clothed human digitization. In:\nProceedings of the IEEE International Conference on Computer Vision. pp. 2304{\n2314 (2019)\n43. Santesteban, I., Otaduy, M.A., Casas, D.: Learning-Based Animation of Clothing\nfor Virtual Try-On. Computer Graphics Forum (Proc. Eurographics) (2019)\n44. Starck, J., Hilton, A.: Surface capture for performance-based animation. IEEE\nComputer Graphics and Applications 27(3), 21{31 (2007)\n45. Stuyck, T.: Cloth Simulation for Computer Graphics. Synthesis Lectures on Visual\nComputing, Morgan & Claypool Publishers (2018)\n46. Tao, Y., Zheng, Z., Zhong, Y., Zhao, J., Quionhai, D., Pons-Moll, G., Liu, Y.:\nSimulcap : Single-view human performance capture with cloth simulation. In: IEEE\nConference on Computer Vision and Pattern Recognition (CVPR) (jun 2019)\n47. Tung, T., Nobuhara, S., Matsuyama, T.: Complete multi-view reconstruction of\ndynamic scenes from probabilistic fusion of narrow and wide baseline stereo. In:\nIEEE 12th International Conference on Computer Vision, ICCV. pp. 1709{1716.\nKyoto, Japan (Sep 2009)\n48. Wang, H., Hecht, F., Ramamoorthi, R., O'Brien, J.F.: Example-based wrinkle\nsynthesis for clothing animation. ACM Transactions on Graphics (Proceedings of\nSIGGRAPH) 29(4), 107:1{8 (Jul 2010)\n49. Wang, H., Ramamoorthi, R., O'Brien, J.F.: Data-driven elastic models for cloth:\nModeling and measurement. ACM Transactions on Graphics (Proceedings of SIG-\nGRAPH) 30(4), 71:1{11 (Jul 2011)\n50. Wang, T.Y., Ceylan, D., Popovic, J., Mitra, N.J.: Learning a shared shape space\nfor multimodal garment design. ACM Trans. Graph. 37(6), 1:1{1:14 (2018)\n51. White, R., Crane, K., Forsyth, D.A.: Capturing and animating occluded cloth.\nACM Trans. Graph. 26(3), 34 (2007)\n52. Xiang, D., Joo, H., Sheikh, Y.: Monocular total capture: Posing face, body, and\nhands in the wild. In: Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition. pp. 10965{10974 (2019)\n53. Xu, H., Li, J., Lu, G., Zhang, D., Long, J.: Predicting ready-made garment dressing\n\ft for individuals based on highly reliable examples. Computers & Graphics (2020)\n54. Xu, Y., Zhu, S.C., Tung, T.: Denserac: Joint 3d pose and shape estimation by dense\nrender and compare. In: International Conference on Computer Vision (2019)\n55. Yamaguchi, K.: Parsing clothing in fashion photographs. In: Proceedings of the\n2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). p.\n35703577. CVPR 12, IEEE Computer Society, USA (2012)\n56. Yamaguchi, K., Kiapour, M.H., Berg, T.L.: Paper doll parsing: Retrieving similar\nstyles to parse clothing items. In: IEEE International Conference on Computer\nVision, ICCV 2013, Sydney, Australia, December 1-8, 2013. pp. 3519{3526. IEEE\nComputer Society (2013)\n57. Yang, J., Franco, J.S., H\u0013 etroy-Wheeler, F., Wuhrer, S.: Analyzing clothing layer\ndeformation statistics of 3d human motions. In: Ferrari, V., Hebert, M., Sminchis-\nescu, C., Weiss, Y. (eds.) Computer Vision { ECCV 2018. pp. 245{261. Springer\nInternational Publishing, Cham (2018)\n\n18 Tiwari et al.\n58. Yang, W., Luo, P.and Lin, L.: Clothing co-parsing by joint image segmentation\nand labeling (2014)\n59. Yu, T., Zheng, Z., Guo, K., Zhao, J., Dai, Q., Li, H., Pons-Moll, G., Liu, Y.:\nDoublefusion: Real-time capture of human performances with inner body shapes\nfrom a single depth sensor. In: The IEEE International Conference on Computer\nVision and Pattern Recognition(CVPR). IEEE (June 2018)\n60. Zhang, C., Pujades, S., Black, M., Pons-Moll, G.: Detailed, accurate, human shape\nestimation from clothed 3D scan sequences. In: IEEE CVPR (2017)\n61. Zheng, Z., Yu, T., Wei, Y., Dai, Q., Liu, Y.: Deephuman: 3d human reconstruction\nfrom a single image. In: The IEEE International Conference on Computer Vision\n(ICCV) (October 2019)\n62. Zhu, H., Cao, Y., Jin, H., Chen, W., Du, D., Wang, Z., Cui, S., Han, X.: Deep\nfashion3d: A dataset and benchmark for 3d garment reconstruction from single\nimages. arXiv preprint arXiv:2003.12753 (2020)", "files_in_pdf": []}